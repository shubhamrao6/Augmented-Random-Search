{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 1000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs) \n",
    "        \n",
    "    def observe(self, x):\n",
    "        self.n += 1. \n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "        \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean= self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return(inputs - obs_mean) / obs_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "        \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "        \n",
    "    def sample_deltas(self):\n",
    "        return [ np.random.randn(*self.theta.shape) for i in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollout, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollout:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the poilicy on one specific direction and over episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward ,1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, normalizer, hp):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the pertubations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "            \n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "            \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k:max(r_pos,r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x])[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print(\"Step: \", step, \"Reward: \", reward_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WalkerBase::__init__ start\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HalfCheetahBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "Step:  0 Reward:  -941.3873978082686\n",
      "Step:  1 Reward:  -966.9903093937359\n",
      "Step:  2 Reward:  -974.9923708143724\n",
      "Step:  3 Reward:  -947.9493094491863\n",
      "Step:  4 Reward:  -960.4767882740035\n",
      "Step:  5 Reward:  -939.2682408455645\n",
      "Step:  6 Reward:  -951.0663211274189\n",
      "Step:  7 Reward:  -908.9736527134739\n",
      "Step:  8 Reward:  -907.4904562414202\n",
      "Step:  9 Reward:  -915.3237604841066\n",
      "Step:  10 Reward:  -932.8649505313862\n",
      "Step:  11 Reward:  -919.0843857713112\n",
      "Step:  12 Reward:  -948.8803816568314\n",
      "Step:  13 Reward:  -894.7189998932784\n",
      "Step:  14 Reward:  -875.7281788446218\n",
      "Step:  15 Reward:  -836.1995199844772\n",
      "Step:  16 Reward:  -693.3669219730958\n",
      "Step:  17 Reward:  -305.71617089173174\n",
      "Step:  18 Reward:  -351.10581391140926\n",
      "Step:  19 Reward:  -481.55370988403297\n",
      "Step:  20 Reward:  -431.4625162275279\n",
      "Step:  21 Reward:  -368.6090619432905\n",
      "Step:  22 Reward:  -506.18841974381587\n",
      "Step:  23 Reward:  -370.25146503980164\n",
      "Step:  24 Reward:  -616.308535319383\n",
      "Step:  25 Reward:  -405.6541518900571\n",
      "Step:  26 Reward:  -386.02765633809054\n",
      "Step:  27 Reward:  -308.6779469493477\n",
      "Step:  28 Reward:  -637.1688675754681\n",
      "Step:  29 Reward:  -266.34108960895287\n",
      "Step:  30 Reward:  -727.6489845967171\n",
      "Step:  31 Reward:  -172.0114025977432\n",
      "Step:  32 Reward:  -292.20191047099013\n",
      "Step:  33 Reward:  -171.15478383082095\n",
      "Step:  34 Reward:  -325.2350585462215\n",
      "Step:  35 Reward:  -199.39154117439116\n",
      "Step:  36 Reward:  -226.61764741650003\n",
      "Step:  37 Reward:  -165.1145221544317\n",
      "Step:  38 Reward:  -190.26697288805045\n",
      "Step:  39 Reward:  -188.25477916661887\n",
      "Step:  40 Reward:  -2.046420969365391\n",
      "Step:  41 Reward:  -64.24776524287603\n",
      "Step:  42 Reward:  24.809809070754717\n",
      "Step:  43 Reward:  39.96321460001522\n",
      "Step:  44 Reward:  -34.446069801832984\n",
      "Step:  45 Reward:  0.9584881117111468\n",
      "Step:  46 Reward:  33.68042208806356\n",
      "Step:  47 Reward:  -419.2852080651812\n",
      "Step:  48 Reward:  49.900936081088645\n",
      "Step:  49 Reward:  33.961587854820515\n",
      "Step:  50 Reward:  100.50514822189095\n",
      "Step:  51 Reward:  67.03101268776769\n",
      "Step:  52 Reward:  57.360600590538176\n",
      "Step:  53 Reward:  114.37354335123395\n",
      "Step:  54 Reward:  159.53900193066346\n",
      "Step:  55 Reward:  -143.70762078136414\n",
      "Step:  56 Reward:  259.04289498596654\n",
      "Step:  57 Reward:  158.07890575224343\n",
      "Step:  58 Reward:  -192.4659988793815\n",
      "Step:  59 Reward:  273.39813524647695\n",
      "Step:  60 Reward:  81.03765716707912\n",
      "Step:  61 Reward:  147.58708712016923\n",
      "Step:  62 Reward:  257.4050294547297\n",
      "Step:  63 Reward:  258.7779861638012\n",
      "Step:  64 Reward:  270.4029266677643\n",
      "Step:  65 Reward:  261.63048400246475\n",
      "Step:  66 Reward:  289.3751174705157\n",
      "Step:  67 Reward:  275.1870912602117\n",
      "Step:  68 Reward:  335.30332499263443\n",
      "Step:  69 Reward:  264.73479170645015\n",
      "Step:  70 Reward:  332.8409941503343\n",
      "Step:  71 Reward:  413.4206136707585\n",
      "Step:  72 Reward:  383.48187922631587\n",
      "Step:  73 Reward:  397.41001649368354\n",
      "Step:  74 Reward:  348.9475277205339\n",
      "Step:  75 Reward:  437.6785496294604\n",
      "Step:  76 Reward:  422.09332618968114\n",
      "Step:  77 Reward:  294.48562977479475\n",
      "Step:  78 Reward:  378.4394401840188\n",
      "Step:  79 Reward:  403.4234890184509\n",
      "Step:  80 Reward:  410.0162406084237\n",
      "Step:  81 Reward:  429.86002661320776\n",
      "Step:  82 Reward:  490.9500958662739\n",
      "Step:  83 Reward:  473.69938558549177\n",
      "Step:  84 Reward:  556.6461394287271\n",
      "Step:  85 Reward:  409.2669165118303\n",
      "Step:  86 Reward:  488.9127039474861\n",
      "Step:  87 Reward:  425.25555197854214\n",
      "Step:  88 Reward:  417.29543370856294\n",
      "Step:  89 Reward:  485.58874636375816\n",
      "Step:  90 Reward:  469.9436539983284\n",
      "Step:  91 Reward:  469.58045675983567\n",
      "Step:  92 Reward:  471.17664215697164\n",
      "Step:  93 Reward:  446.697004074937\n",
      "Step:  94 Reward:  492.3601922140237\n",
      "Step:  95 Reward:  526.4788138805893\n",
      "Step:  96 Reward:  443.8115396179068\n",
      "Step:  97 Reward:  489.6120703212842\n",
      "Step:  98 Reward:  489.73065423241667\n",
      "Step:  99 Reward:  -128.48796430637532\n",
      "Step:  100 Reward:  568.9220942577736\n",
      "Step:  101 Reward:  569.3929824900121\n",
      "Step:  102 Reward:  578.7964444436958\n",
      "Step:  103 Reward:  570.785950536512\n",
      "Step:  104 Reward:  467.66538406069503\n",
      "Step:  105 Reward:  585.7163269470499\n",
      "Step:  106 Reward:  564.6595626140303\n",
      "Step:  107 Reward:  467.54233150481514\n",
      "Step:  108 Reward:  549.0463804792867\n",
      "Step:  109 Reward:  530.8786242193985\n",
      "Step:  110 Reward:  510.96966617396566\n",
      "Step:  111 Reward:  519.3040392421539\n",
      "Step:  112 Reward:  529.0228634904851\n",
      "Step:  113 Reward:  566.3310806576379\n",
      "Step:  114 Reward:  595.5209442432978\n",
      "Step:  115 Reward:  508.7350228638885\n",
      "Step:  116 Reward:  575.9938517726034\n",
      "Step:  117 Reward:  546.7761840709339\n",
      "Step:  118 Reward:  532.3649726652536\n",
      "Step:  119 Reward:  548.9779142052648\n",
      "Step:  120 Reward:  627.2595240905614\n",
      "Step:  121 Reward:  614.8047558720108\n",
      "Step:  122 Reward:  588.2971669578832\n",
      "Step:  123 Reward:  603.1141963561653\n",
      "Step:  124 Reward:  602.7778099943589\n",
      "Step:  125 Reward:  655.0516925516184\n",
      "Step:  126 Reward:  651.7166478876869\n",
      "Step:  127 Reward:  649.2087344679932\n",
      "Step:  128 Reward:  627.6856060011144\n",
      "Step:  129 Reward:  613.1788800697994\n",
      "Step:  130 Reward:  655.678806693623\n",
      "Step:  131 Reward:  647.1409192923421\n",
      "Step:  132 Reward:  673.4981048129665\n",
      "Step:  133 Reward:  676.5922279256647\n",
      "Step:  134 Reward:  622.0968595201858\n",
      "Step:  135 Reward:  681.9176280807195\n",
      "Step:  136 Reward:  636.241235283754\n",
      "Step:  137 Reward:  649.2325981917764\n",
      "Step:  138 Reward:  675.0928714153927\n",
      "Step:  139 Reward:  677.8589365668292\n",
      "Step:  140 Reward:  699.5625434410887\n",
      "Step:  141 Reward:  685.204186699318\n",
      "Step:  142 Reward:  666.2479693328713\n",
      "Step:  143 Reward:  671.8085148562816\n",
      "Step:  144 Reward:  619.2620191033496\n",
      "Step:  145 Reward:  645.8598458580956\n",
      "Step:  146 Reward:  662.7073597566953\n",
      "Step:  147 Reward:  622.9140285769427\n",
      "Step:  148 Reward:  643.0466394211219\n",
      "Step:  149 Reward:  652.6225825581871\n",
      "Step:  150 Reward:  679.9849729979868\n",
      "Step:  151 Reward:  703.1103953831873\n",
      "Step:  152 Reward:  715.1658898664102\n",
      "Step:  153 Reward:  705.6401631285395\n",
      "Step:  154 Reward:  695.7787133137952\n",
      "Step:  155 Reward:  711.5972761770406\n",
      "Step:  156 Reward:  708.1921190199708\n",
      "Step:  157 Reward:  711.9807518498732\n",
      "Step:  158 Reward:  707.2458864004723\n",
      "Step:  159 Reward:  662.344117945597\n",
      "Step:  160 Reward:  683.8803403703397\n",
      "Step:  161 Reward:  720.37826821274\n",
      "Step:  162 Reward:  695.9919728452878\n",
      "Step:  163 Reward:  679.5338288405745\n",
      "Step:  164 Reward:  713.3003075536135\n",
      "Step:  165 Reward:  671.5285927344647\n",
      "Step:  166 Reward:  682.1647276141883\n",
      "Step:  167 Reward:  666.8554400677524\n",
      "Step:  168 Reward:  709.1161388731527\n",
      "Step:  169 Reward:  742.1758204540289\n",
      "Step:  170 Reward:  699.5036511021905\n",
      "Step:  171 Reward:  731.246880663822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c51abb5bd8da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-886a78d6b7bb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, policy, normalizer, hp)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Getting the negative rewards in the negative/opposite directions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_directions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mnegative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"negative\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Gathering all the positive/negative rewards to compute the standard deviation of these rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-846b2485e3c3>\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msum_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnum_plays\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0ee15b6a2dfe>\u001b[0m in \u001b[0;36mobserve\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mlast_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_diff\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlast_mean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_diff\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "\n",
    "hp = Hp()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
